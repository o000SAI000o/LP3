{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb574b-abce-40ae-935b-fabec657e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# UBER FARE PREDICTION (Complete Mini Project)\n",
    "# --------------------------------------------------------------\n",
    "# Problem Statement:\n",
    "# Predict the price of an Uber ride using ML techniques.\n",
    "#\n",
    "# Tasks Covered:\n",
    "# 1. Pre-process the dataset\n",
    "# 2. Identify and remove outliers\n",
    "# 3. Check correlation between features\n",
    "# 4. Apply Linear Regression & Random Forest Regression\n",
    "# 5. Evaluate using R2, RMSE and compare models\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# -----------------------------\n",
    "# 1. LOAD & PRE-PROCESS DATA\n",
    "# -----------------------------\n",
    "\n",
    "'''The dataset is first loaded and the pickup_datetime column is converted into proper datetime format.\n",
    "We then extracted useful time-based features (month, day, hour, weekday) which help in understanding how time affects fare.\n",
    "Using the Haversine formula, we calculated the total trip distance from pickup to drop location.\n",
    "Finally, missing values were removed to ensure clean and reliable data for model training.'''\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"./uber.csv\")        # <- put the Kaggle dataset here\n",
    "\n",
    "# Convert pickup_datetime to proper datetime type\n",
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], errors='coerce')\n",
    "\n",
    "# Extract useful time features (Feature Engineering)\n",
    "df['pickup_month'] = df['pickup_datetime'].dt.month\n",
    "df['pickup_day'] = df['pickup_datetime'].dt.day\n",
    "df['pickup_hour'] = df['pickup_datetime'].dt.hour\n",
    "df['pickup_dayofweek'] = df['pickup_datetime'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "\n",
    "# Function to calculate distance between pickup & drop (Haversine Formula)\n",
    "'''This block defines a function to calculate the geographical distance between pickup \n",
    "and drop-off points using the Haversine formula, which computes the shortest distance \n",
    "between two coordinates on a spherical surface (Earth).\n",
    "Inside the function, map(np.radians, ...) converts latitude and longitude values from \n",
    "degrees to radians for trigonometric calculations. Variables dlat and dlon store the \n",
    "difference between coordinates, and the formula computes the value c, which is multiplied \n",
    "by Earth's radius (6371 km) to get the distance in kilometers.\n",
    "A new feature column distance_km is then created by applying this function to the dataset\n",
    ", and df.dropna() removes any rows containing missing values to ensure clean input data for the model.'''\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Convert degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    # Haversine formula\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return 6371 * c  # 6371 km is Earth radius\n",
    "\n",
    "\n",
    "# Create new feature: total trip distance\n",
    "df['distance_km'] = haversine(df['pickup_latitude'], df['pickup_longitude'],\n",
    "                              df['dropoff_latitude'], df['dropoff_longitude'])\n",
    "\n",
    "\n",
    "# Remove missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(\"After preprocessing shape:\", df.shape)\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2. OUTLIER DETECTION\n",
    "# -----------------------------\n",
    "\n",
    "'''1️⃣ Before Outlier Removal – Boxplot Explanation\n",
    "The boxplot of the fare_amount column shows that there are several data points that lie far outside the normal range.\n",
    "These extreme values are plotted as dots beyond the whiskers of the boxplot.\n",
    "Such values are considered outliers, and they can negatively affect model performance because they mislead the learning algorithm.\n",
    "'''\n",
    "\n",
    "# Boxplot before removing outliers\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.boxplot(x=df['fare_amount'])\n",
    "plt.title(\"Fare Amount - Before Outlier Removal\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''We removed unrealistic fare values (less than 0 or above 200) and trips with incorrect distance (0 km or more than 50 km).\n",
    "This helps clean the data by eliminating invalid and extreme values so the model can learn more accurately.'''\n",
    "\n",
    "# Remove extreme fare values that are unrealistic\n",
    "df = df[(df['fare_amount'] > 0) & (df['fare_amount'] < 200)]\n",
    "\n",
    "# Remove outliers for distance also\n",
    "df = df[(df['distance_km'] > 0) & (df['distance_km'] < 50)]\n",
    "\n",
    "print(\"After removing outliers shape:\", df.shape)\n",
    "\n",
    "'''The second boxplot shows the fare_amount column after removing the extreme values.\n",
    "The number of dots (outliers) has significantly reduced, and the data is now more compact and closer to a normal range.\n",
    "'''\n",
    "\n",
    "# Boxplot after removing outliers\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.boxplot(x=df['fare_amount'], color='orange')\n",
    "plt.title(\"Fare Amount - After Outlier Removal\")\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 3. CHECK CORRELATION\n",
    "# -----------------------------\n",
    "'''This code computes the correlation matrix for selected numerical features using the .corr() function, which measures the strength and direction of the linear relationship between variables.\n",
    "By printing corr['fare_amount'], we specifically view how strongly each feature correlates with the target variable (fare_amount).\n",
    "A heatmap is then plotted using plt.imshow(), where colors (from cmap='coolwarm') visually represent correlation values, and axis labels are set using .xticks() and .yticks() for readability.\n",
    "This visualization helps identify which features have a strong positive or negative impact on fare prediction and supports feature selection for the model.'''\n",
    "\n",
    "corr = df[['fare_amount', 'distance_km', 'pickup_hour', 'pickup_dayofweek', 'pickup_month']].corr()\n",
    "print(\"\\nCorrelation with Fare Amount:\\n\", corr['fare_amount'])\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.imshow(corr, cmap='coolwarm', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(corr.columns)), corr.columns, rotation=45)\n",
    "plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 4. MODEL TRAINING\n",
    "# -----------------------------\n",
    "'''This section prepares the selected feature set (X) and target variable (y) for model \n",
    "training. The train_test_split() function divides the data into training (80%) and testing (20%) \n",
    "subsets to allow the model to learn and then be evaluated on unseen data.\n",
    "A ColumnTransformer with StandardScaler is applied to normalize feature values for Linear Regression, \n",
    "improving model stability and convergence.\n",
    "The Linear Regression model is created using a Pipeline, combining scaling and model training in a single \n",
    "workflow, while RandomForestRegressor is initialized with 200 decision trees to learn non-linear patterns.\n",
    "Both models are trained using .fit(X_train, y_train), enabling them to learn relationships between input \n",
    "features and the target fare amount.'''\n",
    "\n",
    "# Select final features for model\n",
    "features = ['distance_km', 'pickup_hour', 'pickup_dayofweek', 'pickup_month']\n",
    "X = df[features]\n",
    "y = df['fare_amount']\n",
    "\n",
    "# Train-Test Split (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling data for Linear Regression (improves model)\n",
    "scaler = ColumnTransformer([('scale', StandardScaler(), features)], remainder='passthrough')\n",
    "\n",
    "# Linear Regression Model\n",
    "lr_model = Pipeline([('scaler', scaler), ('model', LinearRegression())])\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Random Forest Model\n",
    "rf_model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. MODEL EVALUATION\n",
    "# -----------------------------\n",
    "'''This section evaluates the model performance by generating predictions for the test set using the .predict() method for both Linear Regression and Random Forest models.\n",
    "The custom rmse() function calculates the Root Mean Squared Error (RMSE) by first computing Mean Squared Error (MSE) using mean_squared_error() and then taking its square root to measure average prediction error.\n",
    "The r2_score() metric is used to compute the R² value, which indicates how well the model explains the variance of the target variable (higher R² = better fit).\n",
    "Finally, the R² and RMSE scores for both models are printed, allowing a clear comparison of prediction accuracy and model performance.'''\n",
    "\n",
    "# Predict values\n",
    "pred_lr = lr_model.predict(X_test)\n",
    "pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluation Metrics\n",
    "def rmse(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    return mse ** 0.5     # manually compute RMSE\n",
    "\n",
    "print(\"\\n--- MODEL PERFORMANCE ---\")\n",
    "print(\"Linear Regression → R2:\", r2_score(y_test, pred_lr), \" | RMSE:\", rmse(y_test, pred_lr))\n",
    "print(\"Random Forest     → R2:\", r2_score(y_test, pred_rf), \" | RMSE:\", rmse(y_test, pred_rf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
